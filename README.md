# Boris_Petkovic_SPA_9

# Tema projekta:Kreirati model koji moze da odgovora na pitanja iz datog teksta ili ti QA(question answering) model

# Biblioteke korištene u projektu: 

# Pandas:
-Glavni cilj Pandas biblioteke je pružiti efikasan, fleksibilan i visokonivou apstrakciju za rad sa strukturiranim podacima, posebno tabelarnim podacima. Ova biblioteka često se koristi u analizi podataka, statistici, mašinskom učenju, finansijama i drugim oblastima.<br>


-DataFrame: Osnovna struktura podataka u Pandas-u. To je dvodimenzionalna tabela sa redovima i kolonama, slična SQL tabelama ili Excel radnim listovima.<br>


-Učitavanje i Snimanje Podataka: Pandas pruža funkcionalnosti za učitavanje podataka iz različitih izvora kao što su CSV fajlovi, Excel fajlovi, SQL baze podataka, JSON i drugi formata. Takođe, omogućava snimanje DataFrame-a u različite formate fajlova<br>

# Torch:
-Torch je biblioteka za mašinsko učenje otvorenog koda razvijena od strane Facebook AI Research (FAIR) laboratorije. Njena osnovna svrha je pružiti alate i resurse za rad sa neuronskim mrežama, a posebno za duboko učenje (deep learning). Torch pruža fleksibilan i efikasan okvir za implementaciju različitih modela mašinskog učenja.<br>


-Tensori: Osnovna struktura podataka u Torch-u su tensori, koji su slični NumPy array-ima, ali imaju dodatne funkcionalnosti prilagođene za rad sa GPU-om. Tensori omogućavaju efikasno izračunavanje na velikim skupovima podataka<br>

# Transformers:
-Transformers je biblioteka otvorenog koda koja se fokusira na pružanje alata za rad sa modelima transformatora u oblasti obrade prirodnog jezika (NLP) i drugim sekvencijalnim podacima.<br>


-Transformers pruža prethodno trenirane modele za različite zadatke, kao što su BERT, GPT, RoBERTa, i drugi. Ove arhitekture mogu se koristiti kao osnova za fine-tjuning na specifičnim zadacima ili za generisanje novog teksta<br>


-Biblioteka sadrži alate za tokenizaciju teksta, što je ključno za pripremu podataka za treniranje transformer modela. Tokenizacija je posebno važna jer transformeri rade sa tokenima umesto sa celim reči<br>


-Transformers se lako integriše sa popularnim radnim okruženjima za mašinsko učenje, kao što su PyTorch i TensorFlow. <br>








